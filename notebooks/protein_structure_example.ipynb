{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Secondary Structure Prediction Example\n",
    "\n",
    "This notebook demonstrates the complete workflow for protein secondary structure prediction using our LSTM model. It covers:\n",
    "1. Data preprocessing and vocabulary creation\n",
    "2. Model training and evaluation\n",
    "3. Visualization of results\n",
    "4. Making predictions on new sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "sys.path.append('../scripts')  # Add scripts directory to path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import our custom modules\n",
    "from protein_struct_preprocess import preprocess_proteins\n",
    "from protein_struct_model_prep import proteinDataset, pad_mask\n",
    "from protein_struct_model import lstm_model, protein_train, evaluate\n",
    "from protein_plots import evaluate_with_visualizations\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "train_df = \"../dataset/training_secondary_structure_train.csv\"\n",
    "val_df = \"../dataset/validation_secondary_structure_valid.csv\"\n",
    "test_df = \"../dataset/test_secondary_structure_casp12.csv\"\n",
    "\n",
    "# Preprocess training data to create vocabularies\n",
    "print(\"Preprocessing training data...\")\n",
    "t_data, t_prime2idx, t_lab3, t_lab8 = preprocess_proteins(train_df)\n",
    "\n",
    "# Load validation and test data\n",
    "v_data = pd.read_csv(val_df)\n",
    "ts_data = pd.read_csv(test_df)\n",
    "\n",
    "print(f\"Training data shape: {t_data.shape}\")\n",
    "print(f\"Validation data shape: {v_data.shape}\")\n",
    "print(f\"Test data shape: {ts_data.shape}\")\n",
    "print(f\"Amino acid vocabulary size: {len(t_prime2idx)}\")\n",
    "print(f\"3-class labels: {t_lab3}\")\n",
    "print(f\"8-class labels: {t_lab8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sample training data:\")\n",
    "print(t_data.head())\n",
    "\n",
    "# Analyze sequence length distribution\n",
    "seq_lengths = t_data['seq'].str.len()\n",
    "print(f\"\\nSequence length statistics:\")\n",
    "print(f\"Mean: {seq_lengths.mean():.2f}\")\n",
    "print(f\"Median: {seq_lengths.median():.2f}\")\n",
    "print(f\"Min: {seq_lengths.min()}\")\n",
    "print(f\"Max: {seq_lengths.max()}\")\n",
    "\n",
    "# Plot sequence length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(seq_lengths, bins=50, alpha=0.7)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Protein Sequence Lengths')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and DataLoader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup collate function for padding and masking\n",
    "collate = lambda batch: pad_mask(batch, x_pad=t_prime2idx, y_pad=t_lab3)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = proteinDataset(\n",
    "    data=t_data, \n",
    "    prime2idx=t_prime2idx, \n",
    "    lab3=t_lab3, \n",
    "    lab8=t_lab8, \n",
    "    label_mode=\"q8\",  # Use 8-class prediction\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "val_dataset = proteinDataset(\n",
    "    data=v_data, \n",
    "    prime2idx=t_prime2idx, \n",
    "    lab3=t_lab3, \n",
    "    lab8=t_lab8, \n",
    "    label_mode='q8', \n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "test_dataset = proteinDataset(\n",
    "    data=ts_data, \n",
    "    prime2idx=t_prime2idx, \n",
    "    lab3=t_lab3, \n",
    "    lab8=t_lab8, \n",
    "    label_mode='q8', \n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Created datasets and data loaders\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LSTM model\n",
    "model = lstm_model(\n",
    "    vocab_size=t_prime2idx, \n",
    "    num_tags=len(t_lab8),  # Number of output classes\n",
    "    pad_id=t_prime2idx[\"<PAD>\"],\n",
    "    hidden=20,  # Hidden dimension\n",
    "    embed_dim=20,  # Embedding dimension\n",
    "    bidir=True  # Use bidirectional LSTM\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "trained_model, best_model_state = protein_train(\n",
    "    lab3=t_lab8,  # Use 8-class labels\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    n_factors=30, \n",
    "    n_epochs=20, \n",
    "    batch_size=512, \n",
    "    label_mode='q8', \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on test data\n",
    "print(\"Evaluating model on test data...\")\n",
    "evaluate(\n",
    "    lab3=t_lab8, \n",
    "    test_loader=test_loader, \n",
    "    model=trained_model, \n",
    "    batch_size=512, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations using the trained model\n",
    "print(\"Generating comprehensive visualizations...\")\n",
    "evaluate_with_visualizations(\n",
    "    model_path=\"best_model_state_for_label8.pth\", \n",
    "    test_data_path=test_df, \n",
    "    prime2idx=t_prime2idx, \n",
    "    lab3=t_lab8, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"All visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Making Predictions on New Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_structure(sequence, model, prime2idx, lab3, device=\"mps\"):\n",
    "    \"\"\"\n",
    "    Predict secondary structure for a new protein sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence (str): Amino acid sequence\n",
    "        model: Trained model\n",
    "        prime2idx (dict): Amino acid to index mapping\n",
    "        lab3 (dict): Label to index mapping\n",
    "        device (str): Device to use\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (predicted_labels, confidence_scores)\n",
    "    \"\"\"\n",
    "    # Convert sequence to indices\n",
    "    seq_indices = [prime2idx.get(aa.upper(), prime2idx[\"<UNK>\"]) for aa in sequence]\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    seq_tensor = torch.tensor([seq_indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create mask (all 1s since no padding)\n",
    "    mask = torch.ones_like(seq_tensor, dtype=torch.bool)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get model predictions\n",
    "        logits = model(seq_tensor)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get predicted labels and confidence scores\n",
    "        pred_indices = logits.argmax(-1)[0]\n",
    "        confidence_scores = probs.max(-1)[0]\n",
    "        \n",
    "        # Convert indices to labels\n",
    "        idx2label = {v: k for k, v in lab3.items()}\n",
    "        pred_labels = [idx2label[idx.item()] for idx in pred_indices]\n",
    "        \n",
    "        return pred_labels, confidence_scores.cpu().numpy()\n",
    "\n",
    "# Example usage with a sample sequence\n",
    "sample_sequence = \"ACDEFGHIKLMNPQRSTVWY\"  # Standard 20 amino acids\n",
    "predicted_labels, confidence_scores = predict_structure(\n",
    "    sequence=sample_sequence,\n",
    "    model=trained_model,\n",
    "    prime2idx=t_prime2idx,\n",
    "    lab3=t_lab8,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Sequence: {sample_sequence}\")\n",
    "print(f\"Predicted structure: {''.join(predicted_labels)}\")\n",
    "print(f\"Average confidence: {np.mean(confidence_scores):.3f}\")\n",
    "\n",
    "# Visualize the prediction\n",
    "plt.figure(figsize=(12, 4))\n",
    "structure_colors = {'H': 'red', 'E': 'yellow', 'C': 'grey', 'G': 'blue', 'I': 'green', \n",
    "                   'B': 'orange', 'T': 'purple', 'S': 'pink'}\n",
    "colors = [structure_colors.get(label, 'black') for label in predicted_labels]\n",
    "plt.bar(range(len(predicted_labels)), [1]*len(predicted_labels), color=colors)\n",
    "plt.xticks(range(len(sample_sequence)), list(sample_sequence))\n",
    "plt.xlabel('Amino Acid Position')\n",
    "plt.ylabel('Structure')\n",
    "plt.title('Predicted Secondary Structure')\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different model configurations\n",
    "def compare_models(hidden_dims, embed_dims):\n",
    "    \"\"\"\n",
    "    Compare models with different hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        hidden_dims (list): List of hidden dimensions to test\n",
    "        embed_dims (list): List of embedding dimensions to test\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Comparison results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for hidden in hidden_dims:\n",
    "        for embed in embed_dims:\n",
    "            print(f\"\\nTraining model with hidden={hidden}, embed={embed}...\")\n",
    "            \n",
    "            # Create model\n",
    "            test_model = lstm_model(\n",
    "                vocab_size=t_prime2idx,\n",
    "                num_tags=len(t_lab8),\n",
    "                pad_id=t_prime2idx[\"<PAD>\"],\n",
    "                hidden=hidden,\n",
    "                embed_dim=embed,\n",
    "                bidir=True\n",
    "            )\n",
    "            \n",
    "            # Train for fewer epochs for comparison\n",
    "            _, _ = protein_train(\n",
    "                lab3=t_lab8,\n",
    "                model=test_model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                n_epochs=5,  # Fewer epochs for quick comparison\n",
    "                batch_size=256,\n",
    "                label_mode='q8',\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            test_model.eval()\n",
    "            all_p, all_y = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y, mask in val_loader:\n",
    "                    x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "                    \n",
    "                    logits = test_model(x)\n",
    "                    prob = logits.argmax(-1)\n",
    "                    \n",
    "                    for yi, pi, mi in zip(y, prob, mask):\n",
    "                        yi = yi[mi].tolist()\n",
    "                        pi = pi[mi].tolist()\n",
    "                        all_y.append(yi)\n",
    "                        all_p.append(pi)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            idx2tag = {v: k for k, v in t_lab8.items()}\n",
    "            true_tags = [idx2tag[tag] for sequence in all_y for tag in sequence]\n",
    "            predict_tags = [idx2tag[tag] for sequence in all_p for tag in sequence]\n",
    "            \n",
    "            accuracy = sum(1 for t, p in zip(true_tags, predict_tags) if t == p) / len(true_tags)\n",
    "            \n",
    "            results.append({\n",
    "                'Hidden Dim': hidden,\n",
    "                'Embed Dim': embed,\n",
    "                'Accuracy': accuracy,\n",
    "                'Parameters': sum(p.numel() for p in test_model.parameters())\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run comparison (commented out to save time)\n",
    "# comparison_results = compare_models([10, 20], [10, 20])\n",
    "# print(comparison_results)\n",
    "\n",
    "# Plot comparison results\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for embed_dim in [10, 20]:\n",
    "#     subset = comparison_results[comparison_results['Embed Dim'] == embed_dim]\n",
    "#     plt.plot(subset['Hidden Dim'], subset['Accuracy'], 'o-', label=f'Embed={embed_dim}')\n",
    "# \n",
    "# plt.xlabel('Hidden Dimension')\n",
    "# plt.ylabel('Validation Accuracy')\n",
    "# plt.title('Model Performance vs. Architecture')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated the complete workflow for protein secondary structure prediction:\n",
    "\n",
    "1. **Data Preprocessing**: Created vocabularies for amino acids and structure labels\n",
    "2. **Model Training**: Trained a bidirectional LSTM with early stopping\n",
    "3. **Evaluation**: Assessed model performance on test data\n",
    "4. **Visualization**: Generated comprehensive visualizations of model behavior\n",
    "5. **Prediction**: Made predictions on new sequences\n",
    "\n",
    "### Potential Improvements:\n",
    "- Try different architectures (Transformers, CNNs)\n",
    "- Implement hyperparameter tuning\n",
    "- Use pre-trained protein embeddings\n",
    "- Add attention mechanisms\n",
    "- Experiment with different loss functions\n",
    "\n",
    "### Applications:\n",
    "- Protein function prediction\n",
    "- Drug target identification\n",
    "- Protein engineering\n",
    "- Structure validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}